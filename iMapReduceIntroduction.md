## Abstract ##

Iterative computation is pervasive in many applications such as data mining or social network analysis. These iterative applications typically involve massive data sets containing at least millions or billions of data records. This poses demand of distributed computing frameworks for processing massive data sets on a cluster of machines. MapReduce is an example of such a framework. However, MapReduce lacks built-in support for iterative process that requires to parse data sets iteratively. Besides specifying MapReduce jobs, users have to write a driver program that submits multiple jobs and performs convergence testing at the client. WE presents iMapReduce, a framework that supports iterative processing. iMapReduce allows users to specify the iterative computation with the separated map and reduce functions, and provides the support of automatic iterative processing without the need of a user-defined driver program. More importantly, iMapReduce significantly improves the performance of iterative implementations by (1) reducing the overhead of creating new MapReduce jobs repeatedly, (2) eliminating the shuffling of static data, and (3) allowing asynchronous execution of map tasks. We implement an iMapReduce prototype based on Apache Hadoop, and show that iMapReduce can achieve a factor of 1.1 to 5 speedup over Hadoop for implementing iterative algorithms.

## Limitations of MapReduce Iterative Implementations ##

  1. The operations in each iteration are the same. Nevertheless, MapReduce implementation starts a new job for each iteration, which involves multiple task initializations and cleanups, even though these tasks perform the same operations. Moreover, these jobs have to load the input data from DFS and dump the output data to DFS repeatedly. These can result in the unnecessary **scheduling overhead**.
  1. To maintain the adjacency information, it is shuffled in each iteration between maps and reduces, despite the fact that it remains the same during all iterations. An alternative approach is to run an additional MapReduce job to perform a join operation between the static adjacency data and the dynamic iterated data before each iteration. Both of these methods will result in the unnecessary **communication overhead**.
  1. The map tasks in an iteration cannot be started before finishing all the reduce tasks in the previous iteration. The main loop in the MapReduce implementation requires the completion of previous iteration job before starting the next iteration job. However, the map tasks should start as soon as their input data are available. This limitation results in the unnecessary **synchronization overhead**.

## Iterative Processing ##

<img src='http://rio.ecs.umass.edu/~yzhang/pic/imr.png ' width='300' />

In the MapReduce implementations of iterative algorithms, a series of MapReduce jobs consisting of map operation and reduce operation are scheduled. Figure (a) shows the dataflow in the MapReduce implementation. Each MapReduce job has to load the input data from DFS before the map operation. After the map operation derives the intermediate key-value pairs, the reduce function operates on the intermediate data and derives the output of this iteration, which is written to DFS. In the next iteration, it loads the iterated data from DFS again and repeats the process. Finally, the iterative process terminates when the termination condition is satisfied. These MapReduce jobs including their component map/reduce tasks incur unnecessary scheduling overhead. Additionally, the repeated DFS loading/dumping is expensive, even though Hadoop provides locality optimization that reduces the remote communication.

We note that each iteration performs the same operations. In other words, the series of jobs perform the same map and reduce operations. We exploit this property in iMapReduce by making map/reduce tasks _persistent_. A map/reduce task assigned with a subset of data records has a map/reduce operation. That is, the map/reduce operations in map/reduce tasks are kept executing till the iteration is terminated. Further, iMapReduce enables the reduce operation's output to be passed to the map operation for the next round iteration. Figure (b) shows the dataflow in iMapReduce. The dashed line indicates that the data loading from DFS happens only once in the initialization stage, and the output data are written to DFS only once when the iteration terminates.

## Data Management ##

To avoid the shuffling of unchanged graph data, iMapReduce differentiates the _static data_ from the _state data_. The state data are updated in each iteration, while the static data remain the same across iterations. For example, in the SSSP example, the nodes' shortest distance values as the state data are updated at each iteration, while the link weights as the static data are unchanged. In PageRank, the pages' ranking scores as the state data are updated iteratively, while the adjacency lists as the static data are unchanged.

<img src='http://rio.ecs.umass.edu/~yzhang/pic/itermr.png ' width='250' />

This figure shows the state/static data flow in an iMapReduce worker. Within a worker, the initial state data and the static data are loaded to local FS from DFS in the initial stage. Before each map-reduce iteration starts, The iterated state data are joined with the local static data for map operation. Then the state data produced by map are shuffled to reduce, and the updated state data from reduce are passed to map to start another iteration.

## Asynchronous Map Task Execution ##

In MapReduce iterative algorithm implementations, two synchronizations are existed between maps-reduces and between MapReduce jobs, respectively. Due to the synchronization between jobs, the map tasks of the next iteration job cannot start before the completion of the previous iteration job, which requires all the reduce tasks' completion. However, since the map task needs only the state data from its corresponding reduce task, a map task can start its execution as soon as its input state data arrives, without waiting for the other reduce tasks' completion. In iMapReduce, we schedule the execution of map tasks asynchronously. By enabling the asynchronous execution, the synchronization points between MapReduce jobs are eliminated, which can further speed up the iterative process.